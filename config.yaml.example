# Example configuration showing model-specific context limits

# API keys configuration
openai:
  api_key_env: OPENAI_API_KEY

gemini:
  api_key_env: GOOGLE_API_KEY

anthropic:
  api_key_env: ANTHROPIC_API_KEY

xai:
  api_key_env: XAI_API_KEY

# Model configuration with context limits
models:
  # Graph building model - can have very large context
  # Used for graph building iterations (NOT for initial graph discovery/design)
  graph:
    # Large context window recommended
    # GPT-4.1 WORKS WELL HERE BUT IS VERY EXPENSIVE!
    # provider: openai
    # model: gpt-4.1 
    # max_context: 1000000
    provider: openai
    model: gpt-5-nano
    max_context: 256000
    
  # Scout/agent model for exploration
  scout:
    provider: openai
    model: gpt-5-mini
    max_context: 256000
    # reasoning_effort: low
  
  # Strategic thinking model
  strategist:
    provider: openai
    model: gpt-5
    max_context: 256000
    plan_reasoning_effort: medium
    hypothesize_reasoning_effort: high
  
  # Finalization model
  finalize:
    provider: openai
    model: gpt-5
    reasoning_effort: high
    # No max_context specified - will use global default
    # reasoning_effort: medium  # Options: low, medium, high (GPT-5 models only)
  
  # Reporting model
  reporting:
    provider: openai
    model: gpt-4o
    # No max_context specified - will use global default

# Global context settings (used when model doesn't specify max_context)
context:
  max_tokens: 256000          # Default for models without specific max_context
  compression_threshold: 0.75  # Compress history when 75% full

# Timeouts and retries
timeouts:
  request_seconds: 30

retries:
  max_attempts: 1
  backoff_min_seconds: 1
  backoff_max_seconds: 2

# Notes:
# - Each model can have its own max_context setting
# - Graph model uses large context for building iterations (sees more code)
# - Guidance model uses smaller context for initial discovery/design phase
# - Discovery phase and building iterations use different context limits
# - Models without max_context use the global context.max_tokens value
# - Token counting is done using the specific model for accuracy